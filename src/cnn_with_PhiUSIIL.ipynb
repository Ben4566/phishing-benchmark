{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas muss importiert sein!\n",
    "from benchmark import PerformanceMonitor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# --- 1. FUNKTIONS-DEFINITION (Muss oben stehen!) ---\n",
    "def load_data_from_csv(csv_file_path):\n",
    "    \"\"\"\n",
    "    Lädt URLs und Labels aus einer CSV-Datei.\n",
    "    Erwartet eine CSV mit Spalten für URL und Label.\n",
    "    \"\"\"\n",
    "    print(f\"Lade Daten aus: {csv_file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # CSV einlesen\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # --- WICHTIG: SPALTENNAMEN PRÜFEN ---\n",
    "        # Passen Sie diese Namen an Ihre CSV an (z.B. 'url', 'label', 'type' etc.)\n",
    "        url_col = 'URL'      \n",
    "        label_col = 'label'  \n",
    "        \n",
    "        # Prüfen, ob Spalten existieren\n",
    "        if url_col not in df.columns or label_col not in df.columns:\n",
    "            print(f\"FEHLER: Spalten '{url_col}' oder '{label_col}' nicht in CSV gefunden.\")\n",
    "            print(f\"Vorhandene Spalten: {df.columns.tolist()}\")\n",
    "            return [], np.array([])\n",
    "\n",
    "        # Leere Zeilen entfernen\n",
    "        df = df.dropna(subset=[url_col, label_col])\n",
    "        \n",
    "        # URLs als Liste\n",
    "        urls = df[url_col].astype(str).tolist()\n",
    "        \n",
    "        # Labels verarbeiten\n",
    "        if pd.api.types.is_numeric_dtype(df[label_col]):\n",
    "            labels = df[label_col].values\n",
    "        else:\n",
    "            print(\"Wandle Text-Labels in Zahlen um...\")\n",
    "            label_mapping = {'phishing': 1, 'bad': 1, 'malicious': 1, \n",
    "                             'benign': 0, 'good': 0, 'legitimate': 0}\n",
    "            # Unbekannte Labels werden zu NaN\n",
    "            labels = df[label_col].map(label_mapping)\n",
    "            \n",
    "            if labels.isna().any():\n",
    "                print(f\"Warnung: {labels.isna().sum()} Labels konnten nicht zugeordnet werden und werden entfernt.\")\n",
    "                # Maske für gültige Labels\n",
    "                mask = labels.notna()\n",
    "                urls = [u for u, m in zip(urls, mask) if m]\n",
    "                labels = labels.dropna()\n",
    "            \n",
    "            labels = labels.values.astype(int)\n",
    "            \n",
    "        print(f\"Erfolgreich geladen: {len(urls)} URLs.\")\n",
    "        return urls, np.array(labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "# --- 2. KONFIGURATION & HAUPTPROGRAMM ---\n",
    "MAX_LEN = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Laufe auf: {device}\")\n",
    "\n",
    "# Dateipfad\n",
    "CSV_PATH = r\"..\\data\\processed\\PhiUSIIL_Phishing_URL_Dataset.csv\" \n",
    "\n",
    "# JETZT erst die Funktion aufrufen\n",
    "raw_urls, labels = load_data_from_csv(CSV_PATH)\n",
    "\n",
    "# Überprüfen, ob Daten da sind\n",
    "if len(raw_urls) > 0:\n",
    "    # Split into Train and Test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        raw_urls, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training on {len(X_train)} URLs, Testing on {len(X_test)} URLs.\")\n",
    "else:\n",
    "    print(\"ACHTUNG: Keine Daten geladen. Bitte Dateipfad und Spaltennamen prüfen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. VECTORIZATION (Manuell für PyTorch) ---\n",
    "# Wir bauen ein einfaches Vokabular (Zeichen-basiert), genau wie Keras TextVectorization\n",
    "chars = sorted(list(set(\"\".join(X_train[:1000])))) # Schnelles Vocab aus den ersten 1000 URLs\n",
    "char_to_int = {c: i+2 for i, c in enumerate(chars)} # +2 für Padding (0) und UNK (1)\n",
    "vocab_size = len(char_to_int) + 2\n",
    "\n",
    "def encode_urls(urls, max_len=MAX_LEN):\n",
    "    encoded_batch = []\n",
    "    for url in urls:\n",
    "        # Zeichen zu Int konvertieren\n",
    "        vec = [char_to_int.get(c, 1) for c in url] # 1 = Unknown\n",
    "        # Padding oder Truncating\n",
    "        if len(vec) < max_len:\n",
    "            vec += [0] * (max_len - len(vec))\n",
    "        else:\n",
    "            vec = vec[:max_len]\n",
    "        encoded_batch.append(vec)\n",
    "    return np.array(encoded_batch)\n",
    "\n",
    "print(\"Vektorisiere Daten (das dauert kurz)...\")\n",
    "X_train_enc = torch.tensor(encode_urls(X_train), dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_enc = torch.tensor(encode_urls(X_test), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DataLoader erstellen (für Batching)\n",
    "train_loader = DataLoader(TensorDataset(X_train_enc, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_enc, y_test_tensor), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. DAS CNN MODELL (PyTorch Version) ---\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Conv Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1) # Global Max Pooling\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len] -> [batch, seq_len, embed]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Conv1D erwartet [batch, channels, seq_len], wir müssen die Dimensionen tauschen\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.global_pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten für Dense Layer: [batch, 64, 1] -> [batch, 64]\n",
    "        x = x.squeeze(-1) \n",
    "        \n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Modell initialisieren und auf GPU schieben\n",
    "model = CNNModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "monitor = PerformanceMonitor(\"CNN PhiUSIIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33dc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. TRAINING ---\n",
    "print(\"Starte Training...\")\n",
    "monitor.start_measurement()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} fertig.\")\n",
    "\n",
    "monitor.end_measurement(task_name=\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. INFERENZ & EVALUIERUNG ---\n",
    "print(\"Starte Inferenz (gesamtes Testset)...\")\n",
    "monitor.start_measurement()\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        # Vorhersage\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Daten sammeln (auf CPU schieben für sklearn)\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Zeit stoppen ist hier wichtig, bevor wir Metriken berechnen (außer wir wollen die Rechenzeit der Metriken mitmessen)\n",
    "# Wir messen hier die reine Inferenzzeit des Modells + Data Transfer.\n",
    "\n",
    "# --- METRIKEN BERECHNEN ---\n",
    "# Konvertierung in Arrays\n",
    "y_true = np.array(all_labels)\n",
    "y_scores = np.array(all_preds) # Wahrscheinlichkeiten (Sigmoid Output)\n",
    "y_pred_binary = (y_scores > 0.5).astype(int) # Hard predictions (0 oder 1)\n",
    "\n",
    "# 1. Accuracy\n",
    "acc = accuracy_score(y_true, y_pred_binary)\n",
    "# 2. Precision\n",
    "prec = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 3. Recall\n",
    "rec = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 4. F1 Score\n",
    "f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 5. AUC\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# 6. False Positive Rate (FPR)\n",
    "# Confusion Matrix: tn, fp, fn, tp\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "# Ergebnisse zusammenpacken\n",
    "metrics = {\n",
    "    \"accuracy\": round(acc, 4),\n",
    "    \"precision\": round(prec, 4),\n",
    "    \"recall\": round(rec, 4),\n",
    "    \"f1_score\": round(f1, 4),\n",
    "    \"auc\": round(auc, 4),\n",
    "    \"fpr\": round(fpr, 4)\n",
    "}\n",
    "\n",
    "# An Monitor übergeben\n",
    "monitor.end_measurement(task_name=\"Inferenz\", extra_metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
