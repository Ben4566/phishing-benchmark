{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e0b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laufe auf: cuda\n",
      "Lade Daten aus: ..\\data\\processed\\PhiUSIIL_Phishing_URL_Dataset.csv...\n",
      "Erfolgreich geladen: 235795 URLs.\n",
      "Training on 188636 URLs, Testing on 47159 URLs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas muss importiert sein!\n",
    "from benchmark import PerformanceMonitor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# --- 1. FUNKTIONS-DEFINITION (Muss oben stehen!) ---\n",
    "def load_data_from_csv(csv_file_path):\n",
    "    \"\"\"\n",
    "    Lädt URLs und Labels aus einer CSV-Datei.\n",
    "    Erwartet eine CSV mit Spalten für URL und Label.\n",
    "    \"\"\"\n",
    "    print(f\"Lade Daten aus: {csv_file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # CSV einlesen\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # --- WICHTIG: SPALTENNAMEN PRÜFEN ---\n",
    "        # Passen Sie diese Namen an Ihre CSV an (z.B. 'url', 'label', 'type' etc.)\n",
    "        url_col = 'URL'      \n",
    "        label_col = 'label'  \n",
    "        \n",
    "        # Prüfen, ob Spalten existieren\n",
    "        if url_col not in df.columns or label_col not in df.columns:\n",
    "            print(f\"FEHLER: Spalten '{url_col}' oder '{label_col}' nicht in CSV gefunden.\")\n",
    "            print(f\"Vorhandene Spalten: {df.columns.tolist()}\")\n",
    "            return [], np.array([])\n",
    "\n",
    "        # Leere Zeilen entfernen\n",
    "        df = df.dropna(subset=[url_col, label_col])\n",
    "        \n",
    "        # URLs als Liste\n",
    "        urls = df[url_col].astype(str).tolist()\n",
    "        \n",
    "        # Labels verarbeiten\n",
    "        if pd.api.types.is_numeric_dtype(df[label_col]):\n",
    "            labels = df[label_col].values\n",
    "        else:\n",
    "            print(\"Wandle Text-Labels in Zahlen um...\")\n",
    "            label_mapping = {'phishing': 1, 'bad': 1, 'malicious': 1, \n",
    "                             'benign': 0, 'good': 0, 'legitimate': 0}\n",
    "            # Unbekannte Labels werden zu NaN\n",
    "            labels = df[label_col].map(label_mapping)\n",
    "            \n",
    "            if labels.isna().any():\n",
    "                print(f\"Warnung: {labels.isna().sum()} Labels konnten nicht zugeordnet werden und werden entfernt.\")\n",
    "                # Maske für gültige Labels\n",
    "                mask = labels.notna()\n",
    "                urls = [u for u, m in zip(urls, mask) if m]\n",
    "                labels = labels.dropna()\n",
    "            \n",
    "            labels = labels.values.astype(int)\n",
    "            \n",
    "        print(f\"Erfolgreich geladen: {len(urls)} URLs.\")\n",
    "        return urls, np.array(labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "# --- 2. KONFIGURATION & HAUPTPROGRAMM ---\n",
    "MAX_LEN = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Laufe auf: {device}\")\n",
    "\n",
    "# Dateipfad\n",
    "CSV_PATH = r\"..\\data\\processed\\PhiUSIIL_Phishing_URL_Dataset.csv\" \n",
    "\n",
    "# JETZT erst die Funktion aufrufen\n",
    "raw_urls, labels = load_data_from_csv(CSV_PATH)\n",
    "\n",
    "# Überprüfen, ob Daten da sind\n",
    "if len(raw_urls) > 0:\n",
    "    # Split into Train and Test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        raw_urls, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Training on {len(X_train)} URLs, Testing on {len(X_test)} URLs.\")\n",
    "else:\n",
    "    print(\"ACHTUNG: Keine Daten geladen. Bitte Dateipfad und Spaltennamen prüfen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232f9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektorisiere Daten (das dauert kurz)...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. VECTORIZATION (Manuell für PyTorch) ---\n",
    "# Wir bauen ein einfaches Vokabular (Zeichen-basiert), genau wie Keras TextVectorization\n",
    "chars = sorted(list(set(\"\".join(X_train[:1000])))) # Schnelles Vocab aus den ersten 1000 URLs\n",
    "char_to_int = {c: i+2 for i, c in enumerate(chars)} # +2 für Padding (0) und UNK (1)\n",
    "vocab_size = len(char_to_int) + 2\n",
    "\n",
    "def encode_urls(urls, max_len=MAX_LEN):\n",
    "    encoded_batch = []\n",
    "    for url in urls:\n",
    "        # Zeichen zu Int konvertieren\n",
    "        vec = [char_to_int.get(c, 1) for c in url] # 1 = Unknown\n",
    "        # Padding oder Truncating\n",
    "        if len(vec) < max_len:\n",
    "            vec += [0] * (max_len - len(vec))\n",
    "        else:\n",
    "            vec = vec[:max_len]\n",
    "        encoded_batch.append(vec)\n",
    "    return np.array(encoded_batch)\n",
    "\n",
    "print(\"Vektorisiere Daten (das dauert kurz)...\")\n",
    "X_train_enc = torch.tensor(encode_urls(X_train), dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_enc = torch.tensor(encode_urls(X_test), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DataLoader erstellen (für Batching)\n",
    "train_loader = DataLoader(TensorDataset(X_train_enc, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_enc, y_test_tensor), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c75fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. DAS CNN MODELL (PyTorch Version) ---\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Conv Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1) # Global Max Pooling\n",
    "        \n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len] -> [batch, seq_len, embed]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Conv1D erwartet [batch, channels, seq_len], wir müssen die Dimensionen tauschen\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.global_pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten für Dense Layer: [batch, 64, 1] -> [batch, 64]\n",
    "        x = x.squeeze(-1) \n",
    "        \n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Modell initialisieren und auf GPU schieben\n",
    "model = CNNModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "monitor = PerformanceMonitor(\"CNN PhiUSIIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33dc8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mCNNModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Conv1D erwartet [batch, channels, seq_len], wir müssen die Dimensionen tauschen\u001b[39;00m\n\u001b[32m     27\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool1(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     30\u001b[39m x = \u001b[38;5;28mself\u001b[39m.global_pool(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Flatten für Dense Layer: [batch, 64, 1] -> [batch, 64]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\GitHub Repositorys\\Evaluation_of_AI_Models\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    360\u001b[39m         F.pad(\n\u001b[32m    361\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. TRAINING ---\n",
    "print(\"Starte Training...\")\n",
    "monitor.start_measurement()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} fertig.\")\n",
    "\n",
    "monitor.end_measurement(task_name=\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. INFERENZ & EVALUIERUNG ---\n",
    "print(\"Starte Inferenz (gesamtes Testset)...\")\n",
    "monitor.start_measurement()\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        # Vorhersage\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Daten sammeln (auf CPU schieben für sklearn)\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Zeit stoppen ist hier wichtig, bevor wir Metriken berechnen (außer wir wollen die Rechenzeit der Metriken mitmessen)\n",
    "# Wir messen hier die reine Inferenzzeit des Modells + Data Transfer.\n",
    "\n",
    "# --- METRIKEN BERECHNEN ---\n",
    "# Konvertierung in Arrays\n",
    "y_true = np.array(all_labels)\n",
    "y_scores = np.array(all_preds) # Wahrscheinlichkeiten (Sigmoid Output)\n",
    "y_pred_binary = (y_scores > 0.5).astype(int) # Hard predictions (0 oder 1)\n",
    "\n",
    "# 1. Accuracy\n",
    "acc = accuracy_score(y_true, y_pred_binary)\n",
    "# 2. Precision\n",
    "prec = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 3. Recall\n",
    "rec = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 4. F1 Score\n",
    "f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "# 5. AUC\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# 6. False Positive Rate (FPR)\n",
    "# Confusion Matrix: tn, fp, fn, tp\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "# Ergebnisse zusammenpacken\n",
    "metrics = {\n",
    "    \"accuracy\": round(acc, 4),\n",
    "    \"precision\": round(prec, 4),\n",
    "    \"recall\": round(rec, 4),\n",
    "    \"f1_score\": round(f1, 4),\n",
    "    \"auc\": round(auc, 4),\n",
    "    \"fpr\": round(fpr, 4)\n",
    "}\n",
    "\n",
    "# An Monitor übergeben\n",
    "monitor.end_measurement(task_name=\"Inferenz\", extra_metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
